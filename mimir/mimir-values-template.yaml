# These values configure the Grafana Mimir or Grafana Enterprise Metrics cluster
# for a more production-ready setup. The setup targets 70% CPU and memory utilization
# so that the cluster has room to grow. The resource requests reflect 70% utilization
# and the limits reflect 100% utilization. The values do not set CPU limits,
# because CPU limits have caused severe issues elsewhere, so we don't apply any in our helm chart:
# see https://engineering.indeedblog.com/blog/2019/12/unthrottled-fixing-cpu-limits-in-the-cloud/
# If you require CPU limits for billing purposes see capped-large.yaml
#
# These values are suitable for ingestion of ~10M series and scrape interval of 15s.
# This implies ingestion rate of around 660000 samples per second.
#
# Query requirements can vary dramatically depending on query rate and query
# ranges. The values here satisfy a "usual" query load of around 50 queries per second
# as seen from our production clusters at this scale.
#
# The values in this file also add podAntiAffinity rules for ingesters and store-gateways.
# The rules ensure that the replicas of the same component are not scheduled on the same
# Kubernetes Node. Zone-aware replication is enabled by default on new installation.
# Refer to [Migrate from single zone to zone-aware replication with Helm](https://grafana.com/docs/mimir/latest/migration-guide/migrating-from-single-zone-with-helm) and
# [Zone-Aware Replication](https://grafana.com/docs/mimir/latest/configure/configure-zone-aware-replication/)
# for more information.
#
# MinIO is no longer enabled, and you are encouraged to use your cloud providers
# object storage service such as S3 or GCS.

# Contains values for production use for ingestion up to approximately ten million series.

# image:
#   # -- Grafana Mimir container image repository. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.repository'
#   repository: grafana/mimir
#   # -- Grafana Mimir container image tag. Note: for Grafana Enterprise Metrics use the value 'enterprise.image.tag'
#   tag: r343-b21e239
#   # -- Container pull policy - shared between Grafana Mimir and Grafana Enterprise Metrics
#   pullPolicy: IfNotPresent




# https://grafana.com/docs/mimir/latest/manage/run-production-environment/
# https://github.com/grafana/mimir/blob/main/operations/mimir-mixin-compiled/dashboards/mimir-compactor-resources.json

alertmanager:
  enabled: true
  zoneAwareReplication:
    enabled: false
  statefulSet:
    enabled: true
  persistentVolume:
    enabled: true
    size: 1Gi
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain
    accessModes:
      - ReadWriteOnce
  replicas: 1 #3
  resources:
    limits:
      memory: 1.4Gi
    requests:
      cpu: 500m #1
      memory: 500Mi #1Gi

compactor:
  enabled: true
  replicas: 1
  persistentVolume:
    enabled: true
    size: 5Gi #50Gi
    accessModes:
      - ReadWriteOnce
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain
  resources:
    limits:
      memory: 2Gi #2.8Gi
    requests:
      cpu: 500m #1
      memory: 500Mi #2Gi

distributor:
  enabled: true
  podDisruptionBudget:
    maxUnavailable: 1
  replicas: 1 #12
  resources:
    limits:
      memory: 1Gi #5.7Gi
    requests:
      cpu: 1 #2
      memory: 1Gi #4Gi

# https://grafana.com/docs/mimir/latest/manage/run-production-environment/planning-capacity/#ingester
# For Ingester Estimated required CPU, memory, and disk space:

# CPU: 1 core for every 300,000 series in memory
# Memory: 2.5GB for every 300,000 series in memory
# Disk space: 5GB for every 300,000 series in memory
# How to estimate the number of series in memory:
# Query the number of active series across all your Prometheus servers:
# sum(prometheus_tsdb_head_series)
# Check the configured -ingester.ring.replication-factor (defaults to 3)
# Estimate the total number of series in memory across all ingesters using the following formula:
# total number of in-memory series = <active series> * <replication factor>
ingester:
  enabled: true
  replicas: 2
  zoneAwareReplication:
    enabled: false
  statefulSet:
    enabled: true
  podDisruptionBudget:
    maxUnavailable: 1
  persistentVolume:
    enabled: true
    size: 5Gi #50Gi
    name: storage
    accessModes:
      - ReadWriteOnce
    enableRetentionPolicy: false
    whenDeleted: Retain
    whenScaled: Retain
  # replicas: 2 #27
  resources:
    limits:
      memory: 2Gi #12Gi
    requests:
      cpu: 500m #3.5
      memory: 1Gi #8Gi
  topologySpreadConstraints: {}
  # affinity:
  #   podAntiAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       - labelSelector:
  #           matchExpressions:
  #             - key: target # support for enterprise.legacyLabels
  #               operator: In
  #               values:
  #                 - ingester
  #         topologyKey: 'kubernetes.io/hostname'

  #       - labelSelector:
  #           matchExpressions:
  #             - key: app.kubernetes.io/component
  #               operator: In
  #               values:
  #                 - ingester
  #         topologyKey: 'kubernetes.io/hostname'

  # zoneAwareReplication:
  #   topologyKey: 'kubernetes.io/hostname'

admin-cache:
  enabled: false
  replicas: 1 #3

chunks-cache:
  enabled: true
  replicas: 1 #3
  batchSize: 4
  parallelism: 5
  timeout: 2000ms
  defaultValidity: 0s
  port: 11211
  # -- Amount of memory allocated to chunks-cache for object storage (in MB).
  allocatedMemory: 512 #8192  # resources limit & request of memcached object is calculated using allocatedMemory * 1.2
  # -- Maximum item memory for chunks-cache (in MB).
  maxItemMemory: 5
  # -- Maximum number of connections allowed
  connectionLimit: 16384

query_scheduler:
  enabled: true
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
  # podDisruptionBudget:
  #   maxUnavailable: 1

index-cache:
  enabled: true
  replicas: 1 #3
  port: 11211
  allocatedMemory: 512 #2048
  maxItemMemory: 5
  connectionLimit: 16384

metadata-cache:
  enabled: true
  replicas: 1 #3
  port: 11211
  allocatedMemory: 300
  maxItemMemory: 1 #(in MB)
  connectionLimit: 16384
  # just used for now as i don't have much resources available, later will remove this resource quota as we are using allocatedMemory.
  resources:
    requests:
      cpu: "100m"
      memory: "200Mi"
    limits:
      cpu: "300m"
      memory: "300Mi"

results-cache:
  enabled: true
  replicas: 1 #3
  port: 11211
  allocatedMemory: 512
  maxItemMemory: 5
  connectionLimit: 16384

minio:
  enabled: false

overrides_exporter:
  enabled: true
  replicas: 1
  podDisruptionBudget:
    maxUnavailable: 1
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

querier:
  enabled: true
  replicas: 1 #4
  resources:
    limits:
      memory: 1Gi #8.5Gi
    requests:
      cpu: 500m #2
      memory: 500Mi #6Gi

query_frontend:
  enabled: true
  replicas: 1 #3
  podDisruptionBudget:
    maxUnavailable: 1
  resources:
    limits:
      memory: 2.8Gi
    requests:
      cpu: 500m #2
      memory: 500Mi #2Gi

ruler:
  enabled: true
  replicas: 1 #3
  podDisruptionBudget:
    maxUnavailable: 1
  resources:
    limits:
      memory: 2Gi #5.7Gi
    requests:
      cpu: 500m
      memory: 1Gi #4Gi

store_gateway:
  enabled: true
  persistentVolume:
    size: 5Gi #50Gi
  replicas: 1 #6
  resources:
    limits:
      memory: 1Gi #8.5Gi
    requests:
      cpu: 300m #1
      memory: 300Mi #6Gi
  topologySpreadConstraints: {}
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: target # support for enterprise.legacyLabels
                operator: In
                values:
                  - store-gateway
          topologyKey: 'kubernetes.io/hostname'

        - labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/component
                operator: In
                values:
                  - store-gateway
          topologyKey: 'kubernetes.io/hostname'
  zoneAwareReplication: 
    enabled: false
  # zoneAwareReplication:
  #   topologyKey: 'kubernetes.io/hostname'

nginx:
  replicas: 1 #3
  resources:
    limits:
      memory: 731Mi
    requests:
      cpu: 500m #1
      memory: 512Mi
  image:
    registry: docker.io
    repository: nginxinc/nginx-unprivileged
    tag: 1.28-alpine
  # -- Basic auth configuration
  basicAuth:
    # -- Enables basic authentication for nginx
    enabled: true
    existingSecret: mimir-basic-auth

  config:
    # -- NGINX log format
    logFormat: |-
      main '$remote_addr - $remote_user [$time_local]  $status '
              '"$request" $body_bytes_sent "$http_referer" '
              '"$http_user_agent" "$http_x_forwarded_for"';
    # -- Sets the log level of the NGINX error log. One of `debug`, `info`, `notice`, `warn`, `error`, `crit`, `alert`, or `emerg`
    errorLogLevel: error
    # -- Enables NGINX access logs
    accessLogEnabled: true
    # -- Allows appending custom configuration to the server block
    serverSnippet: ""
    # -- Allows appending custom configuration to the http block
    httpSnippet: ""
    # -- Allow to set client_max_body_size in the nginx configuration
    clientMaxBodySize: 540M
    # -- Allows to set a custom resolver
    resolver: null
    # -- Configures whether or not NGINX bind IPv6
    enableIPv6: true
    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating.
    file: |
      worker_processes  5;  ## Default: 1
      error_log  /dev/stderr {{ .Values.gateway.nginx.config.errorLogLevel }};
      pid        /tmp/nginx.pid;
      worker_rlimit_nofile 8192;

      events {
        worker_connections  4096;  ## Default: 1024
      }

      http {
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path       /tmp/proxy_temp_path;
        fastcgi_temp_path     /tmp/fastcgi_temp;
        uwsgi_temp_path       /tmp/uwsgi_temp;
        scgi_temp_path        /tmp/scgi_temp;

        default_type application/octet-stream;
        log_format   {{ .Values.gateway.nginx.config.logFormat }}

        {{- if .Values.gateway.nginx.verboseLogging }}
        access_log   /dev/stderr  main;
        {{- else }}

        map $status $loggable {
          ~^[23]  0;
          default 1;
        }
        access_log   {{ .Values.gateway.nginx.config.accessLogEnabled | ternary "/dev/stderr  main  if=$loggable;" "off;" }}
        {{- end }}

        sendfile           on;
        tcp_nopush         on;
        proxy_http_version 1.1;

        {{- if .Values.gateway.nginx.config.resolver }}
        resolver {{ .Values.gateway.nginx.config.resolver }};
        {{- else }}
        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};
        {{- end }}

        {{- with .Values.gateway.nginx.config.httpSnippet }}
        {{ . | nindent 2 }}
        {{- end }}

        # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
        map $http_x_scope_orgid $ensured_x_scope_orgid {
          default $http_x_scope_orgid;
          "" "{{ include "mimir.noAuthTenant" . }}";
        }

        map $http_x_scope_orgid $has_multiple_orgid_headers {
          default 0;
          "~^.+,.+$" 1;
        }

        proxy_read_timeout 300;
        server {
          listen {{ include "mimir.serverHttpListenPort" . }};
          {{- if .Values.gateway.nginx.config.enableIPv6 }}
          listen [::]:{{ include "mimir.serverHttpListenPort" . }};
          {{- end }}

          {{- if .Values.gateway.nginx.config.clientMaxBodySize }}
          client_max_body_size {{ .Values.gateway.nginx.config.clientMaxBodySize }};
          {{- end }}

          {{- if .Values.gateway.nginx.basicAuth.enabled }}
          auth_basic           "Mimir";
          auth_basic_user_file /etc/nginx/secrets/.htpasswd;
          {{- end }}

          if ($has_multiple_orgid_headers = 1) {
              return 400 'Sending multiple X-Scope-OrgID headers is not allowed. Use a single header with | as separator instead.';
          }

          location = / {
            return 200 'OK';
            auth_basic off;
          }

          location = /ready {
            return 200 'OK';
            auth_basic off;
          }

          proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;

          # Distributor endpoints
          location /distributor {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /api/v1/push {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location /otlp/v1/metrics {
            set $distributor {{ template "mimir.fullname" . }}-distributor-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$distributor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Alertmanager endpoints
          location {{ template "mimir.alertmanagerHttpPrefix" . }} {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /multitenant_alertmanager/status {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /multitenant_alertmanager/configs {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /api/v1/alerts {
            set $alertmanager {{ template "mimir.fullname" . }}-alertmanager-headless.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$alertmanager:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Ruler endpoints
          location {{ template "mimir.prometheusHttpPrefix" . }}/config/v1/rules {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/rules {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          location {{ template "mimir.prometheusHttpPrefix" . }}/api/v1/alerts {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }
          location = /ruler/ring {
            set $ruler {{ template "mimir.fullname" . }}-ruler.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$ruler:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Rest of {{ template "mimir.prometheusHttpPrefix" . }} goes to the query frontend
          location {{ template "mimir.prometheusHttpPrefix" . }} {
            set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Buildinfo endpoint can go to any component
          location = /api/v1/status/buildinfo {
            set $query_frontend {{ template "mimir.fullname" . }}-query-frontend.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$query_frontend:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          # Compactor endpoint for uploading blocks
          location /api/v1/upload/block/ {
            set $compactor {{ template "mimir.fullname" . }}-compactor.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }};
            proxy_pass      http://$compactor:{{ include "mimir.serverHttpListenPort" . }}$request_uri;
          }

          {{- with .Values.gateway.nginx.config.serverSnippet }}
          {{ . | nindent 4 }}
          {{- end }}
        }
      }

# Grafana Enterprise Metrics feature related
admin_api:
  enabled: false
  replicas: 1 #2
  resources:
    limits:
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 64Mi

gateway:
  replicas: 1 #3
  resources:
    limits:
      memory: 731Mi
    requests:
      cpu: 500m #1
      memory: 512Mi
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 15%
  # podDisruptionBudget:
  #   maxUnavailable: 1
  containerSecurityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop: [ALL]
  service:
    port: 80
    # -- Type of the Service
    type: ClusterIP
    # -- ClusterIP of the Service
    clusterIP: null
    # -- Node port if service type is NodePort
    nodePort: null
    # -- Load balancer IP address if service type is LoadBalancer
    loadBalancerIP: null
    # -- Annotations for the Service
    annotations: {}
    # -- Labels for the Service
    labels: {}

serviceAccount:
 create: true
 name: mimir
 annotations:
   "eks.amazonaws.com/role-arn": "${mimir_role_arn}" # The service role you created


# Source: mimir-distributed/templates/mimir-config.yaml
mimir:
  structuredConfig:
    common:
      storage:
        backend: s3
        s3:
          endpoint: s3.${region_name}.amazonaws.com
          region: ${region_name}
          insecure: false
    usage_stats:
      enabled: false
      installation_mode: helm
    activity_tracker:
      filepath: /active-query-tracker/activity.log
    alertmanager:
      data_dir: /data
      enable_api: true
      retention: 120h0m0s
      poll_interval: 15s
      max_recv_msg_size: 104857600
      external_url: /alertmanager
      fallback_config_file: /configs/alertmanager_fallback_config.yaml
      peer_timeout: 15s
    alertmanager_storage:
      s3:
        # access_key_id: ${AWS_ACCESS_KEY_ID}
        bucket_name: ${env_mimir_ruler_bucket}
        endpoint: s3.${region_name}.amazonaws.com
        # region: ap-southeast-1
        insecure: false
        # secret_access_key: ${AWS_SECRET_ACCESS_KEY}
    blocks_storage:
      s3:
        bucket_name: ${env_mimir_chunk_bucket}
      bucket_store:
        sync_dir: /data/tsdb-sync
        ignore_blocks_within: 10h0m0s
        chunks_cache:
          backend: memcached
          memcached:
            addresses: dns+mimir-chunks-cache.mimir.svc.cluster.local.:11211
            max_item_size: 5242880 # Calculated from 5MB (5 * 1024 * 1024)
            timeout: 750ms
            max_idle_connections: 150
        index_cache:
          backend: memcached
          memcached:
            addresses: dns+mimir-index-cache.mimir.svc.cluster.local.:11211 # Resolved address and corrected port
            max_item_size: 5242880 # (5 MB) - Standard default if not specified elsewhere
            timeout: 750ms
            max_idle_connections: 150
        metadata_cache:
          backend: memcached
          memcached:
            addresses: dns+mimir-metadata-cache.mimir.svc.cluster.local.:11211 # Resolved address and corrected port
            max_item_size: 5242880 # (5 MB) - Standard default if not specified elsewhere
            max_idle_connections: 150
      tsdb:
        dir: /data/tsdb
        head_compaction_interval: 15m
        wal_replay_concurrency: 3
    compactor:
      block_ranges:
        - 2h0m0s
        - 12h0m0s
        - 24h0m0s
      block_sync_concurrency: 8
      meta_sync_concurrency: 20
      data_dir: /data
      compaction_retries: 3
      compaction_concurrency: 1
      first_level_compaction_wait_period: 25m0s
      cleanup_interval: 15m0s
      cleanup_concurrency: 20
      deletion_delay: 2h0m0s
      tenant_cleanup_delay: 6h0m0s
      max_compaction_time: 1h0m0s
      no_blocks_file_cleanup_enabled: false
      max_opening_blocks_concurrency: 4
      max_closing_blocks_concurrency: 2
      symbols_flushers_concurrency: 4
      max_block_upload_validation_concurrency: 1
      compaction_interval: 30m
      sharding_ring:
        wait_stability_min_duration: 1m
        heartbeat_period: 1m
        heartbeat_timeout: 4m
      compaction_jobs_order: smallest-range-oldest-blocks-first
    distributor:
      max_recv_msg_size: 104857600
      max_otlp_request_size: 104857600
      max_influx_request_size: 104857600
      max_request_pool_buffer_size: 0
      remote_timeout: 2s
      pool:
        client_cleanup_period: 15s
        health_check_ingesters: true
      ring:
        heartbeat_period: 1m
        heartbeat_timeout: 4m
      instance_limits:
        max_ingestion_rate: 0
        max_inflight_push_requests: 2000
        max_inflight_push_requests_bytes: 0
      write_requests_buffer_pooling_enabled: true
      reusable_ingester_push_workers: 2000
      influx_endpoint_enabled: false
      start_time_quiet_zero: false

    frontend:
      log_queries_longer_than: 0s
      max_body_size: 10485760
      split_queries_by_interval: 24h
      query_stats_enabled: true
      parallelize_shardable_queries: true
      results_cache:
        backend: memcached
        memcached:
          timeout: 500ms
          connect_timeout: 200ms
          write_buffer_size_bytes: 4096
          read_buffer_size_bytes: 4096
          min_idle_connections_headroom_percentage: -1
          max_idle_connections: 100
          max_async_concurrency: 50
          max_async_buffer_size: 25000
          max_get_multi_concurrency: 100
          max_get_multi_batch_size: 100
          addresses: dns+mimir-results-cache.mimir.svc.cluster.local.:11211
          max_item_size: 5242880
      cache_results: true
      cache_errors: false
      query_sharding_target_series_per_shard: 2500
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
        grpc_compression: ""
        rate_limit: 0
        rate_limit_burst: 0
        backoff_on_ratelimits: false
        backoff_config:
            min_period: 100ms
            max_period: 10s
            max_retries: 10
        initial_stream_window_size: 63KiB1023B
        initial_connection_window_size: 63KiB1023B
    frontend_worker:
    # frontend address and scheduler address are mutually exclusive
    #     You cannot configure both:
    # frontend_worker.frontend_address
    # frontend_worker.scheduler_address
      scheduler_address: mimir-query-scheduler-headless.mimir.svc.cluster.local:9095
      grpc_client_config:
        max_send_msg_size: 419430400 # 400MiB
        max_recv_msg_size: 104857600
        rate_limit: 0
        rate_limit_burst: 0
        backoff_on_ratelimits: false
        backoff_config:
            min_period: 100ms
            max_period: 10s
            max_retries: 10
        initial_stream_window_size: 63KiB1023B
        initial_connection_window_size: 63KiB1023B
        connect_timeout: 5s
        connect_backoff_base_delay: 1s
        connect_backoff_max_delay: 5s
    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        unregister_on_shutdown: false
        tokens_file_path: /data/tokens
        heartbeat_period: 2m
        heartbeat_timeout: 10m0s
      metadata_retain_period: 10m0s
      rate_update_period: 15s
      active_series_metrics_enabled: true
      active_series_metrics_update_period: 1m0s
      active_series_metrics_idle_timeout: 20m0s
      tsdb_config_update_period: 15s
      # Enable and configure the Push Circuit Breaker
      push_circuit_breaker:
        enabled: true
        thresholding_period: 2m               # Calculate failure rate over 2 minutes
        failure_threshold_percentage: 15      # Open if >15% of pushes fail
        cooldown_period: 30s                  # Stay open for 30 seconds
        request_timeout: 5s                   # Timeout push requests after 5 seconds
      # Enable and configure the Read Circuit Breaker
      read_circuit_breaker:
        enabled: true
        thresholding_period: 1m               # Calculate failure rate over 1 minute
        failure_threshold_percentage: 20      # Open if >20% of reads fail
        cooldown_period: 15s                  # Stay open for 15 seconds
        request_timeout: 45s                  # Timeout read requests after 45 seconds
      instance_limits:
        max_ingestion_rate: 0
        max_tenants: 0
        max_series: 0
        max_inflight_push_requests: 30000
        max_inflight_push_requests_bytes: 0
        # {{- if .Values.ingester.zoneAwareReplication.enabled }}
        # zone_awareness_enabled: true
        # {{- end }}
    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
        grpc_compression: ""
        rate_limit: 0
        rate_limit_burst: 0
        backoff_on_ratelimits: false
        backoff_config:
            min_period: 100ms
            max_period: 10s
            max_retries: 10
        initial_stream_window_size: 63KiB1023B
        initial_connection_window_size: 63KiB1023B
    # By default, metrics that are stored in the object storage are never deleted, and the storage utilization will increase over time.
    # You can configure the object storage retention to automatically delete all metrics data older than the configured period.
    limits:
      # Delete from storage metrics data older than 1 year from object storage.
      compactor_blocks_retention_period: 1y
      compactor_partial_block_deletion_delay: 1d
      compactor_block_upload_enabled: false
      compactor_block_upload_validation_enabled: true
      compactor_block_upload_verify_chunks: true
      compactor_block_upload_max_block_size_bytes: 0
      compactor_in_memory_tenant_meta_cache_size: 0
      compactor_max_lookback: 0s
      # Global default ingestion limits (samples/sec and burst)
      ingestion_rate: 250000       # Adjust based on your overall cluster capacity and typical load
      ingestion_burst_size: 1000000 # Typically 10-20x ingestion_rate
      request_rate: 0
      request_burst_size: 0
      query_ingesters_within: 13h
      out_of_order_time_window: 5m
      max_label_name_length: 1024
      max_label_value_length: 2048
      max_label_names_per_series: 30
      max_label_names_per_info_series: 80
      max_metadata_length: 1024
      max_global_series_per_user: 2500000
      max_native_histogram_buckets: 0
      max_exemplars_per_series_per_request: 0
      reduce_native_histogram_over_max_buckets: true
      creation_grace_period: 10m
      past_grace_period: 0s
      enforce_metadata_metric_name: true
      ingestion_tenant_shard_size: 0
      metric_relabeling_enabled: true
      otel_metric_suffixes_enabled: false
      otel_created_timestamp_zero_ingestion_enabled: false
      promote_otel_resource_attributes: ""
      otel_keep_identifying_resource_attributes: false
      ingest_storage_read_consistency: eventual
      ingestion_partitions_tenant_shard_size: 0

    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+mimir-gossip-ring.mimir.svc.cluster.local:7946
    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.mimir-alertmanager-headless.test.svc.cluster.local/alertmanager
      enable_api: true
      rule_path: /data
    querier:
      max_concurrent: 16
      query_store_after: 12h0m0s
      timeout: 2m0s
      max_samples: 50000000
      default_evaluation_interval: 1m0s
      lookback_delta: 5m0s
      minimize_ingester_requests: true
      minimize_ingester_requests_hedging_delay: 3s
      mimir_query_engine:
        enable_aggregation_operations: true
        enable_binary_logical_operations: true
        enable_one_to_many_and_many_to_one_binary_operations: true
        enable_scalars: true
        enable_scalar_scalar_binary_comparison_operations: true
        enable_subqueries: true
        enable_vector_scalar_binary_comparison_operations: true
        enable_vector_vector_binary_comparison_operations: true
        disabled_aggregations: ""
        disabled_functions: ""
    ruler_storage:
      s3:
        # access_key_id: ${AWS_ACCESS_KEY_ID}
        bucket_name: ${env_mimir_ruler_bucket}
        endpoint: s3.${region_name}.amazonaws.com
        # region: ap-southeast-1
        insecure: false
        # secret_access_key: ${AWS_SECRET_ACCESS_KEY}
    runtime_config:
      file: /var/mimir/runtime.yaml
    server:
      grpc_server_max_concurrent_streams: 1000
      log_source_ips_enabled: true
      # log_source_ips_header: "X-Custom-Client-IP" # Uncomment and modify if needed
      # log_source_ips_regex: "^([^,]+)" # Uncomment and modify if needed
      # log_source_ips_full: true # Uncomment if you want all IPs
    store_gateway:
      sharding_ring:
        heartbeat_period: 1m
        heartbeat_timeout: 10m
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false # does not remove itself from the ring during shutdown.
        num_tokens: 512
        wait_stability_min_duration: 1m0s
        wait_stability_max_duration: 5m0s
    timeseries_unmarshal_caching_optimization_enabled: true
    cost_attribution_eviction_interval: 20m0s
    cost_attribution_registry_path: ""
    cost_attribution_cleanup_interval: 3m0s
    multitenancy_enabled: false

metaMonitoring:
  dashboards:
    enabled: false
    # -- Alternative namespace to create dashboards ConfigMaps in. They are created in the Helm release namespace by default.
    namespace: null
  serviceMonitor:
    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created
    enabled: true
    namespace: monitoring # namespace where prometheus stack is running
    namespaceSelector: null # namespace which this service will look for the services
    interval: 30s
    scheme: http
    labels:
      release: kube-prometheus-stack # label which prometheus is looking for. whether monitor this serviceMonitor or not.






# helm upgrade --install mimir grafana/mimir-distributed -n mimir -f ../mimir/mimir-override-values.yaml

# Release "mimir" does not exist. Installing it now.
# W0606 16:26:51.391720   34621 warnings.go:70] spec.template.spec.containers[0].resources.limits[memory]: fractional byte value "3006477107200m" is invalid, must be an integer
# W0606 16:26:51.666826   34621 warnings.go:70] spec.template.spec.containers[0].resources.limits[memory]: fractional byte value "1503238553600m" is invalid, must be an integer
# NAME: mimir
# LAST DEPLOYED: Fri Jun  6 16:26:33 2025
# NAMESPACE: mimir
# STATUS: deployed
# REVISION: 1
# NOTES:
# Welcome to Grafana Mimir!
# Remote write endpoints for Prometheus or Grafana Agent:
# Ingress is not enabled, see the nginx.ingress values.
# From inside the cluster:
#   http://mimir-nginx.mimir.svc:80/api/v1/push

# Read address, Grafana data source (Prometheus) URL:
# Ingress is not enabled, see the nginx.ingress values.
# From inside the cluster:
#   http://mimir-nginx.mimir.svc:80/prometheus

# **IMPORTANT**: Always consult CHANGELOG.md file at https://github.com/grafana/mimir/blob/main/operations/helm/charts/mimir-distributed/CHANGELOG.md and the deprecation list there to learn about breaking changes that require action during upgrade.